{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMS 3.1（政策文本版）源代码及其操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "d:\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#以下代码块用于支持所有操作的环境搭建\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from paddlenlp import Taskflow\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#装载算法\n",
    "# seg = Taskflow('word_segmentation', user_dict = './userdict/userdict-paddle.txt', mode = 'accurate', batch_size=32)\n",
    "tag = Taskflow('pos_tagging', user_dict ='./dict/user_dict.txt', mode='accurate', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':#传入参数\n",
    "    stop_path = './dict/hit_stopwords.txt'#停用词位置\n",
    "    dict_path = './dict/user_dict.txt'#用户自定义词典\n",
    "    data_path = './按照阶段合并/第二阶段/'#选择要处理的具体阶段\n",
    "    seg_str_path = './S第二阶段seg_str/'#选择要处理的具体阶段对应的分词存储位置\n",
    "    edge_path = tfidf_path = './tfidf/temp_out2.xlsx'\n",
    "    temp_out = './各阶段共词网络/'\n",
    "    model_path = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块为语料预处理做支撑\n",
    "def LL_stop(stop_path): # 装载哈工大中文停用词表\n",
    "    f_stop = open(stop_path,'r', encoding='utf-8') \n",
    "    sw = [line.strip() for line in f_stop]  # 首次遍历停用词表\n",
    "    f_stop.close()  #清空缓存 \n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块用于分词，并返回适用于word2vec列表数据接口（但是非常占内存！！！）\n",
    "filter_list = ['v','n','an']\n",
    "\n",
    "filter_result = []\n",
    "\n",
    "for id in os.listdir(data_path):\n",
    "\n",
    "    pre_cor = open(data_path + id, 'r', encoding='utf-8')\n",
    "    cor = pre_cor.read()\n",
    "    re_tag = [item for item in tag(cor)]\n",
    "    pre_cor.close()#关闭文件\n",
    "    for word,flag in re_tag:\n",
    "        if flag in filter_list:\n",
    "            filter_result.append(word)  \n",
    "LL_fenci = [term for term in filter_result if term not in LL_stop(stop_path) and len(term) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块用于将LL-fenci结果存储至Word2Vec模型训练中，每个循环只可运行一次。\n",
    "cor = ' '.join(LL_fenci)\n",
    "print(cor, file=open(model_path + '/' + 'train_data.txt', 'a', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块读取LL_fenci内容，并将其存储为可供二次查询的信息\n",
    "def write_LL_fenci():\n",
    "    filter_list = ['v','n','an']\n",
    "\n",
    "    for id in os.listdir(data_path):\n",
    "        \n",
    "        filter_result = []\n",
    "\n",
    "        pre_cor = open(data_path + id, 'r', encoding='utf-8')\n",
    "        cor = pre_cor.read()\n",
    "        re_tag = [item for item in tag(cor)]\n",
    "        pre_cor.close()#关闭文件\n",
    "        for word,flag in re_tag:\n",
    "            if flag in filter_list:\n",
    "                filter_result.append(word)  \n",
    "    # LL_fenci = [term for term in filter_result if term not in LL_stop(stop_path) and len(term) > 1]\n",
    "        write_LL_fenci = [term for term in filter_result if term not in LL_stop(stop_path) and len(term) > 1]\n",
    "        ee = open(seg_str_path + '/' + id, 'w', encoding='utf-8')#输出结果\n",
    "        ee.write(' '.join(write_LL_fenci))\n",
    "        ee.close()\n",
    "write_LL_fenci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块用于全部文档级别的关键词tf-idf值的计算与输出；除非更新数据，否则请勿独立运行(现已删除索引列)\n",
    "corpus = []\n",
    "for id in os.listdir(seg_str_path):\n",
    "    contend = open(seg_str_path + id,'r',encoding='utf-8').read()\n",
    "    corpus.append(contend)\n",
    "\n",
    "def tf_idf():\n",
    "    vectorizer = CountVectorizer()#将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "    transformer = TfidfTransformer()#统计每个词语的tf-idf权值\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "\n",
    "    word = vectorizer.get_feature_names_out()\n",
    "    weight = tfidf.toarray()\n",
    "\n",
    "    data = {'word': word,'tfidf': weight.sum(axis=0).tolist()}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    sorted_df = df.sort_values(by=\"tfidf\", ascending = False).iloc[0:100]#对dataframe格式的tfidf矩阵以“by=tfidf”值降序排列，并“iloc[0:60]”输出前60个的值\n",
    "    # print(sorted_df)\n",
    "    sorted_df.to_excel(tfidf_path,index=False)#将sorted_df结果写入到xlsx\n",
    "    # excel = pd.read_excel(tfidf_path, index_col= None)\n",
    "    # excel.drop(excel.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "tf_idf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`******可修改过滤词性**********`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#共现词矩阵模块（1）：以下代码块通过函数Get_check_cor(data_path)用于生成所有文档\"句子级别\"的分词结果，并以二维列表形式返回结果以支持二次遍历\n",
    "tag_filter = ['v', 'n','an']#可在此处修改词性过滤条件\n",
    "\n",
    "data_array = []\n",
    "\n",
    "for id in os.listdir(data_path):\n",
    "    res_list = []\n",
    "    raw_contend = open(data_path + '/' + id, 'r', encoding = 'utf-8').read()\n",
    "    raw_contend = str(re.split('；|。|：|\\n',raw_contend))\n",
    "\n",
    "    seg_result = [item for item in tag(raw_contend)]\n",
    "\n",
    "    for word,flag in seg_result:\n",
    "        if flag in tag_filter:\n",
    "            res_list.append(word)\n",
    "    data_array.append([item for item in res_list if item not in LL_stop(stop_path) and len(item) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "共现词矩阵模块（2）：以下代码准备从新的路径载入tf-idf_excel中的数据；以下代码块用于初始化矩阵和填入共现数据，注意：对角线数据为0\n",
    "def get_edge(edge_path):\n",
    "    edge = []\n",
    "    raw_edge = pd.read_excel(edge_path, sheet_name=0,usecols=[0])\n",
    "    raw_edge = raw_edge.values.tolist()\n",
    "\n",
    "    for i in raw_edge:\n",
    "        edge.append(i[0])\n",
    "\n",
    "    return edge\n",
    "\n",
    "edge = get_edge(edge_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下代码块用于生成共现词网络\n",
    "def matrix(edge):\n",
    "    \n",
    "    matrix = [['' for j in range(len(edge) + 1)] for i in range(len(edge) + 1)]\n",
    "    \n",
    "    matrix[0][1:] = np.array(edge)\n",
    "\n",
    "    matrix = list(map(list, zip(*matrix)))\n",
    "\n",
    "    matrix[0][1:] = np.array(edge)\n",
    "    return matrix\n",
    "\n",
    "matrix = matrix(edge)\n",
    "\n",
    "def count_matrix(matrix, data_array):\n",
    "\n",
    "    for row in range(1, len(matrix)):\n",
    "        for col in range(1, len(matrix)):\n",
    "            if matrix[0][row] == matrix[col][0]:\n",
    "                matrix[col][row] = str(0)\n",
    "\n",
    "            else:\n",
    "                counter = 0\n",
    "\n",
    "                for ech in data_array:\n",
    "                    if matrix[0][row] in ech and matrix[col][0] in ech:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                matrix[col][row] = str(counter)\n",
    "\n",
    "    data1 = pd.DataFrame(matrix)\n",
    "    data1.to_csv(temp_out +'共词.csv', index=0, columns=None, encoding='utf_8_sig', header=False)\n",
    "count_matrix(matrix, data_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*****请确认模型数据，否则不要运行以下所有代码！！！*******`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec：除非更新模型，否则请不要运行这个代码块\n",
    "sentences=word2vec.Text8Corpus(model_path + '/' + 'train_data.txt')\n",
    "model= Word2Vec(sentences, sg=1, vector_size=100,window=5,min_count=3, negative=5, hs=1)\n",
    "model.save(model_path + '/' + 'word2vec_model1.bin')\n",
    "model.wv.save_word2vec_format(model_path + '/' + 'word2vec_model2.txt', binary = \"False\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans(1)：装载word2vec语言模型并进行Kmeans聚类\n",
    "model = Word2Vec.load(model_path + '/' + 'word2vec_model1.bin')\n",
    "\n",
    "keys = model.wv.key_to_index.keys()\n",
    "\n",
    "# print(keys)\n",
    "# print(type(keys))\n",
    "# print(list(keys)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出PCA降维后的特征\n",
    "edge = list(set([line.strip() for line in open('./各阶段共词网络/关键词汇总.txt','r',encoding='utf-8').readlines()]))\n",
    "\n",
    "wordvector = [model.wv[key] for key in keys if key in edge]\n",
    "edge_wordvector = [word for word in keys if word in edge]\n",
    "\n",
    "col_num = []#声明一个列表\n",
    "for num in range(1, 101):\n",
    "    col_num.append(str(num)+'维')\n",
    "\n",
    "df_wordvector = pd.DataFrame(wordvector,index=list(edge_wordvector), columns=list(col_num))\n",
    "\n",
    "df_wordvector.to_csv('你肯定知道我是干啥的.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "版权所有 (c) 2023 [**@Pluckism**]\n",
    "\n",
    "特此授予任何获取本软件及其相关文档文件(以下简称“软件”)的人免费使用的权利，包括但不限于使用、复制、修改、合并、发布、分发、再授权和/或出售软件的权利，并允许被许可人按照上述条件使用软件。\n",
    "\n",
    "在所有软件副本或实质部分中包含以上版权声明和本许可声明。\n",
    "\n",
    "本软件按“原样”提供，不附带任何形式的明示或暗示的保证，包括但不限于适销性保证。在任何情况下，作者或版权持有人均不对因使用软件或无法使用软件而导致的任何直接或间接损失承担责任。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright (c) 2023 [**@Pluckism**]\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd6cb6fd77b58ccead11d532db52aa25d5d2f385114270479a47b62f431339b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
